# Reinforcement Learning Project

University project demonstrating **First-Visit Monte Carlo Prediction** and **Dynamic Programming** for policy evaluation.

## ğŸ“‹ Project Overview

This project implements:

1. **Session Environment Simulator** - Custom discrete environment with stochastic transitions and sparse, delayed rewards
2. **First-Visit Monte Carlo Prediction** - Model-free policy evaluation from sampled episodes
3. **Dynamic Programming** - Model-based policy evaluation and improvement on a toy MDP
4. **Interactive Streamlit Frontend** - Comprehensive web UI for all functionality
5. **CLI Interface** - Command-line execution with automatic visualization

## ğŸ¯ Key Features

- **States**: {0: Passive Browsing, 1: Selective Reading, 2: Deep Engagement, 3: Exit (terminal)}
- **Sparse Rewards**: 
  - +10 if session length â‰¥ 8
  - +4 if session length 4-7  
  - -8 if session length â‰¤ 3
  - All intermediate rewards = 0
- **Discount Factor (Î³)**: 0.9
- **First-Visit Rule**: Each state updated only once per episode
- **Proper Returns Calculation**: Backward pass with discounting

## ğŸ› ï¸ Setup

### Prerequisites

- Python 3.8+ (using existing `.venv` virtual environment)
- Required packages: `numpy`, `matplotlib`, `streamlit`, `plotly`, `pandas`

### Installation

1. Activate your virtual environment:
```bash
source .venv/bin/activate  # Linux/Mac
# or
.venv\Scripts\activate  # Windows
```

2. Install dependencies:
```bash
pip install numpy matplotlib streamlit plotly pandas
```

## ğŸš€ Usage

### Option 1: Streamlit Web Interface (Recommended)

Run the interactive web application:

```bash
streamlit run app.py
```

This opens a browser with 4 interactive pages:

1. **ğŸŒ Environment & Episodes** - Configure and generate episodes
2. **ğŸ“Š Monte Carlo Training** - Train MC and visualize convergence
3. **ğŸ§  Dynamic Programming** - Run policy evaluation/improvement on toy MDP
4. **ğŸ“ˆ Analysis & Comparison** - Compare MC vs DP results

### Option 2: Command Line Interface

Run the complete workflow:

```bash
python main.py
```

This executes:
- Episode generation (2000 episodes by default)
- First-Visit MC prediction
- Dynamic Programming evaluation
- Automatic plotting and result saving

## ğŸ“ Project Structure

```
RL_Project/
â”œâ”€â”€ env.py              # Session environment + episode generation
â”œâ”€â”€ mc.py               # First-Visit Monte Carlo prediction
â”œâ”€â”€ dp.py               # Dynamic Programming (policy eval/improvement)
â”œâ”€â”€ utils.py            # Plotting, saving, and helper functions
â”œâ”€â”€ main.py             # CLI execution script
â”œâ”€â”€ app.py              # Streamlit web application
â”œâ”€â”€ README.md           # This file
â”œâ”€â”€ outputs/            # Generated plots and results
â”‚   â”œâ”€â”€ convergence.png
â”‚   â”œâ”€â”€ episode_lengths.png
â”‚   â””â”€â”€ mc_values.csv
â””â”€â”€ data/               # Saved episodes
    â””â”€â”€ episodes.npy
```

## ğŸ“Š Outputs

### Streamlit App
- Interactive visualizations
- Real-time convergence plots
- Episode explorer
- Download results as JSON/CSV

### CLI
- **outputs/convergence.png** - MC convergence plot for all states
- **outputs/episode_lengths.png** - Episode length histogram with reward boundaries
- **outputs/mc_values.csv** - Final state values
- **data/episodes.npy** - All generated episodes (for reproducibility)

## ğŸ§ª Testing

Each module includes self-tests. Run individually:

```bash
python env.py      # Test environment
python mc.py       # Test MC with hand-crafted episode
python dp.py       # Test DP policy evaluation
python utils.py    # Test utility functions
```

## ğŸ” Implementation Details

### Environment Design

**Transition Probabilities** (designed to create varied session lengths):
- **State 0**: 40% stay, 30% â†’ 1, 20% â†’ 2, 10% â†’ Exit
- **State 1**: 30% â†’ 0, 40% stay, 20% â†’ 2, 10% â†’ Exit
- **State 2**: 10% â†’ 0, 20% â†’ 1, 50% stay, 20% â†’ Exit

State 2 is "stickier" to encourage longer sessions.

### First-Visit Monte Carlo

```
Algorithm:
1. Initialize V(s) = 0 for all states
2. For each episode:
   a. Compute returns G_t using backward pass:
      G = 0
      for t from last to first:
          G = Î³ * G + reward[t]
   b. For each state s in episode:
      if first visit to s:
          append G_t to returns[s]
          V(s) = mean(returns[s])
```

### Dynamic Programming

Separate toy MDP with known transitions:
- **Policy Evaluation**: Bellman expectation iteration
- **Policy Improvement**: Greedy policy w.r.t. computed values

## ğŸ“ Key Differences: MC vs DP

| Aspect | Monte Carlo | Dynamic Programming |
|--------|-------------|---------------------|
| **Model** | Model-free | Model-based (requires P, R) |
| **Learning** | From episodes | From model knowledge |
| **Variance** | Higher | Lower |
| **Efficiency** | Needs more samples | More sample-efficient |
| **Use Case** | Unknown dynamics | Known dynamics |
| **Bootstrap** | No | Yes (uses V estimates) |
| **Task Type** | Episodic only | Both episodic & continuing |

## ğŸ“ Course Requirements

This implementation satisfies all project requirements:

âœ… Discrete states: 0, 1, 2 (non-terminal), 3 (terminal)  
âœ… Sparse, delayed rewards (+10, +4, -8)  
âœ… Discount factor Î³ = 0.9  
âœ… First-Visit MC with proper returns computation  
âœ… Episode generation with all reward buckets  
âœ… Convergence visualization  
âœ… DP bonus (policy evaluation + improvement)  
âœ… Complete documentation

## ğŸ“„ Academic Report

The academic report (`report.pdf`) will be created after implementation verification, covering:
- Environment design and methodology
- MC convergence analysis
- DP results interpretation
- MC vs DP comparison
- Effect of sparse/delayed rewards on learning

## ğŸ¤ Contributing

This is a university project. Modifications should maintain:
- Exact state/reward specifications
- First-visit MC rule
- Proper return computation with Î³=0.9
- Episode termination at state 3

## ğŸ“§ Contact

For questions or issues, refer to the course materials or instructor.

---

**Version**: 1.0  
**Last Updated**: January 2026
